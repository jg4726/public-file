{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jg4726/public-file/blob/main/Transformer_stock_market_stock_market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vsPqfAxUKs8",
        "outputId": "c12cc92e-2d2d-482d-d60b-6f2b634391e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: CPU/TPU\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type='text/css'>\n",
              ".datatable table.frame { margin-bottom: 0; }\n",
              ".datatable table.frame thead { border-bottom: none; }\n",
              ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
              ".datatable .bool    { background: #DDDD99; }\n",
              ".datatable .object  { background: #565656; }\n",
              ".datatable .int     { background: #5D9E5D; }\n",
              ".datatable .float   { background: #4040CC; }\n",
              ".datatable .str     { background: #CC4040; }\n",
              ".datatable .time    { background: #40CC40; }\n",
              ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
              ".datatable .frame tbody td { text-align: left; }\n",
              ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
              ".datatable th:nth-child(2) { padding-left: 12px; }\n",
              ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
              ".datatable .sp {  opacity: 0.25;}\n",
              ".datatable .footer { font-size: 9px; }\n",
              ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
              ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "device = 'GPU' if 'GPU' in tf.test.gpu_device_name() else 'CPU/TPU'\n",
        "print('Device:', device)\n",
        "\n",
        "import os, gc, random, datetime\n",
        "if device == 'GPU':\n",
        "    import cudf\n",
        "    import cupy as cp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import janestreet\n",
        "import xgboost as xgb\n",
        "import datatable as dtable\n",
        "from hyperopt import hp, fmin, tpe, Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from joblib import dump, load\n",
        "from time import time\n",
        "from numba import njit\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxctdZ5yUKtA",
        "outputId": "979004a8-f9d2-4dfe-8e14-5ccee91b8858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afdaulf-UKtB",
        "outputId": "4f1bffd9-38d5-47dd-bfb7-59c90221325e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accelerated Linear Algebra enabled\n"
          ]
        }
      ],
      "source": [
        "MIXED_PRECISION = False\n",
        "XLA_ACCELERATE = True\n",
        "\n",
        "if MIXED_PRECISION:\n",
        "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "    mixed_precision.set_policy(policy)\n",
        "    print('Mixed precision enabled')\n",
        "\n",
        "if XLA_ACCELERATE:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print('Accelerated Linear Algebra enabled')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3kn_J9bUKtC",
        "outputId": "81618a27-a3b3-41b5-9e47-beb30e560775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading...\n",
            "Filling...\n",
            "Finish.\n",
            "Wall time: 7.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "print('Loading...')\n",
        "train = dtable.fread('jane_street_train.jay').to_pandas()\n",
        "features = [c for c in train.columns if 'feature' in c]\n",
        "\n",
        "print('Filling...')\n",
        "train = train.query('weight > 0').reset_index(drop = True)\n",
        "train[features] = train[features].fillna(method = 'ffill').fillna(0)\n",
        "train['action'] = (train['resp'] > 0).astype('int')\n",
        "\n",
        "print('Finish.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFM2ISizUKtC"
      },
      "outputs": [],
      "source": [
        "if 'date' in features:\n",
        "    print('yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1-J9N8DUKtD",
        "outputId": "2c6fe054-3cfd-44d5-d664-ef10349cc239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp',\n",
              "       'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
              "       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n",
              "       'feature_10', 'feature_11', 'feature_12', 'feature_13',\n",
              "       'feature_14', 'feature_15', 'feature_16', 'feature_17',\n",
              "       'feature_18', 'feature_19', 'feature_20', 'feature_21',\n",
              "       'feature_22', 'feature_23', 'feature_24', 'feature_25',\n",
              "       'feature_26', 'feature_27', 'feature_28', 'feature_29',\n",
              "       'feature_30', 'feature_31', 'feature_32', 'feature_33',\n",
              "       'feature_34', 'feature_35', 'feature_36', 'feature_37',\n",
              "       'feature_38', 'feature_39', 'feature_40', 'feature_41',\n",
              "       'feature_42', 'feature_43', 'feature_44', 'feature_45',\n",
              "       'feature_46', 'feature_47', 'feature_48', 'feature_49',\n",
              "       'feature_50', 'feature_51', 'feature_52', 'feature_53',\n",
              "       'feature_54', 'feature_55', 'feature_56', 'feature_57',\n",
              "       'feature_58', 'feature_59', 'feature_60', 'feature_61',\n",
              "       'feature_62', 'feature_63', 'feature_64', 'feature_65',\n",
              "       'feature_66', 'feature_67', 'feature_68', 'feature_69',\n",
              "       'feature_70', 'feature_71', 'feature_72', 'feature_73',\n",
              "       'feature_74', 'feature_75', 'feature_76', 'feature_77',\n",
              "       'feature_78', 'feature_79', 'feature_80', 'feature_81',\n",
              "       'feature_82', 'feature_83', 'feature_84', 'feature_85',\n",
              "       'feature_86', 'feature_87', 'feature_88', 'feature_89',\n",
              "       'feature_90', 'feature_91', 'feature_92', 'feature_93',\n",
              "       'feature_94', 'feature_95', 'feature_96', 'feature_97',\n",
              "       'feature_98', 'feature_99', 'feature_100', 'feature_101',\n",
              "       'feature_102', 'feature_103', 'feature_104', 'feature_105',\n",
              "       'feature_106', 'feature_107', 'feature_108', 'feature_109',\n",
              "       'feature_110', 'feature_111', 'feature_112', 'feature_113',\n",
              "       'feature_114', 'feature_115', 'feature_116', 'feature_117',\n",
              "       'feature_118', 'feature_119', 'feature_120', 'feature_121',\n",
              "       'feature_122', 'feature_123', 'feature_124', 'feature_125',\n",
              "       'feature_126', 'feature_127', 'feature_128', 'feature_129',\n",
              "       'ts_id', 'action'], dtype=object)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWvgSZtKUKtD"
      },
      "source": [
        "*Base Transformer structure from https://www.tensorflow.org/tutorials/text/transformer, modified with Swish activation function.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxYUOnR0UKtF"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        \n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        \n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    \n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation = 'swish'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
        "        \n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training = training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training = training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "                 maximum_position_encoding, rate = 0.1):\n",
        "        \n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.maximum_position_encoding = maximum_position_encoding\n",
        "        self.rate = rate\n",
        "\n",
        "#         self.pos_encoding = positional_encoding(self.maximum_position_encoding, \n",
        "#                                                 self.d_model)\n",
        "#         self.embedding = tf.keras.layers.Dense(self.d_model)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim = self.maximum_position_encoding, \n",
        "                                                 output_dim = self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
        "        \n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_layers': self.num_layers,\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'maximum_position_encoding': self.maximum_position_encoding,\n",
        "            'dropout': self.dropout,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, x, training, mask = None):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "#         x += self.pos_encoding[:, :seq_len, :]\n",
        "#         x = self.embedding(x)\n",
        "        positions = tf.range(start = 0, limit = seq_len, delta = 1)\n",
        "        x += self.pos_emb(positions)\n",
        "\n",
        "        x = self.dropout(x, training = training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDR6BKE1UKtJ"
      },
      "outputs": [],
      "source": [
        "def create_transformer_model(num_columns, num_labels, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate):\n",
        "    \n",
        "    inp = tf.keras.layers.Input(shape = (window_size, num_columns))\n",
        "    x = tf.keras.layers.BatchNormalization()(inp)\n",
        "    x = tf.keras.layers.Dense(d_model)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('swish')(x)\n",
        "    x = tf.keras.layers.SpatialDropout1D(dropout_rate)(x)\n",
        "    x = TransformerEncoder(num_layers, d_model, num_heads, dff, window_size, dropout_rate)(x)\n",
        "    out = tf.keras.layers.Dense(num_labels, activation = 'sigmoid')(x[:, -1, :])\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
        "    model.compile(optimizer = tfa.optimizers.AdamW(weight_decay = weight_decay, learning_rate = learning_rate),\n",
        "                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n",
        "                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n",
        "                 )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBLE-ycNUKtJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 4096 * strategy.num_replicas_in_sync\n",
        "num_layers = 1\n",
        "d_model = 96\n",
        "num_heads = 1\n",
        "dff = 64\n",
        "window_size = 3\n",
        "dropout_rate = 0.15\n",
        "weight_decay = 0\n",
        "label_smoothing = 1e-2\n",
        "learning_rate = 1e-3 * strategy.num_replicas_in_sync\n",
        "verbose = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZUrBc9TUKtK",
        "outputId": "5916a852-6b83-48ad-9f7e-a4f52c632ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3, 130)]          0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 3, 130)           520       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3, 96)             12576     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 3, 96)            384       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation (Activation)     (None, 3, 96)             0         \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 3, 96)            0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, 3, 96)            50368     \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 96)               0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 97        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,945\n",
            "Trainable params: 63,493\n",
            "Non-trainable params: 452\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n",
        "model.summary()\n",
        "\n",
        "K.clear_session()\n",
        "del model\n",
        "rubbish = gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7nDtGNiUKtK"
      },
      "outputs": [],
      "source": [
        "# Use Tensorflow Dataset\n",
        "def prepare_dataset(X, y, window_size, batch_size, mode = 'training'):\n",
        "    x_ds = tf.data.Dataset.from_tensor_slices(X) \n",
        "    y_ds = tf.data.Dataset.from_tensor_slices(y[window_size - 1:])\n",
        "    x_ds = x_ds.window(window_size, shift = 1, drop_remainder = True)\n",
        "    x_ds = x_ds.flat_map(lambda window: window.batch(window_size))\n",
        "    dataset = tf.data.Dataset.zip((x_ds, y_ds))\n",
        "    if mode == 'training':\n",
        "        buffer_size = batch_size * 8\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration = True)\n",
        "        dataset = dataset.batch(batch_size)#, drop_remainder = True\n",
        "    elif mode == 'validation':\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.cache() \n",
        "    elif mode == 'testing':\n",
        "        dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# Use Numpy [may cause Out-of-Memory (OOM) error]\n",
        "def rolling_window(a, shape):  # rolling window for 2D array\n",
        "    s = (a.shape[0] - shape[0] + 1,) + (a.shape[1] - shape[1] + 1,) + shape\n",
        "    strides = a.strides + a.strides\n",
        "    return np.squeeze(np.lib.stride_tricks.as_strided(a, shape = s, strides = strides), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re8xnyAwUKtK"
      },
      "outputs": [],
      "source": [
        "X_tr = train.loc[train['date'] <= 450, features].values\n",
        "y_tr = train.loc[train['date'] <= 450, 'action'].values\n",
        "\n",
        "# X_tr2 = train.loc[(train['date'] >= 303) & (train['date'] <= 367), features].values\n",
        "# y_tr2 = train.loc[(train['date'] >= 303) & (train['date'] <= 367), 'action'].values\n",
        "\n",
        "X_val = train.loc[train['date'] > 450, features].values\n",
        "y_val = train.loc[train['date'] > 450, 'action'].values\n",
        "\n",
        "rubbish = gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozr_HXYLUKtL",
        "outputId": "985de7ec-4980-43fa-96f8-3631381e81f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1748599\n",
            "232688\n"
          ]
        }
      ],
      "source": [
        "print(len(X_tr))\n",
        "# print(len(X_tr2))\n",
        "print(len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP65rrvMUKtL"
      },
      "outputs": [],
      "source": [
        "X_tr = rolling_window(X_tr, (window_size, len(features)))\n",
        "X_val = rolling_window(X_val, (window_size, len(features)))\n",
        "y_tr = y_tr[window_size - 1:]\n",
        "y_val = y_val[window_size - 1:]\n",
        "# X_tr2 = rolling_window(X_tr2, (window_size, len(features)))\n",
        "# y_tr2 = y_tr2[window_size - 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaedHIlAUKtL",
        "outputId": "2f652c12-8ff9-48b2-b5c1-6b31a5a73081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "261526\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pHvzdcUlUKtM",
        "outputId": "7a171506-d8c1-4d4c-ab0a-498dbf1032f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "427/427 [==============================] - 66s 151ms/step - loss: 0.6996 - AUC: 0.5120 - val_loss: 0.6917 - val_AUC: 0.5301 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "427/427 [==============================] - 64s 150ms/step - loss: 0.6921 - AUC: 0.5248 - val_loss: 0.6914 - val_AUC: 0.5309 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "427/427 [==============================] - 64s 151ms/step - loss: 0.6916 - AUC: 0.5280 - val_loss: 0.6914 - val_AUC: 0.5326 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "427/427 [==============================] - 65s 152ms/step - loss: 0.6912 - AUC: 0.5308 - val_loss: 0.6913 - val_AUC: 0.5349 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "427/427 [==============================] - 63s 149ms/step - loss: 0.6908 - AUC: 0.5327 - val_loss: 0.6915 - val_AUC: 0.5358 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "427/427 [==============================] - 65s 151ms/step - loss: 0.6905 - AUC: 0.5339 - val_loss: 0.6912 - val_AUC: 0.5375 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "427/427 [==============================] - 65s 153ms/step - loss: 0.6902 - AUC: 0.5351 - val_loss: 0.6922 - val_AUC: 0.5371 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "427/427 [==============================] - 63s 148ms/step - loss: 0.6898 - AUC: 0.5369 - val_loss: 0.6912 - val_AUC: 0.5374 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "427/427 [==============================] - 65s 151ms/step - loss: 0.6894 - AUC: 0.5382 - val_loss: 0.6914 - val_AUC: 0.5389 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "427/427 [==============================] - 64s 150ms/step - loss: 0.6890 - AUC: 0.5394 - val_loss: 0.6921 - val_AUC: 0.5362 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "427/427 [==============================] - 63s 149ms/step - loss: 0.6886 - AUC: 0.5409 - val_loss: 0.6917 - val_AUC: 0.5377 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "427/427 [==============================] - ETA: 0s - loss: 0.6883 - AUC: 0.5416\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "427/427 [==============================] - 63s 148ms/step - loss: 0.6883 - AUC: 0.5416 - val_loss: 0.6919 - val_AUC: 0.5365 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "427/427 [==============================] - 63s 147ms/step - loss: 0.6875 - AUC: 0.5450 - val_loss: 0.6917 - val_AUC: 0.5385 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "427/427 [==============================] - 63s 149ms/step - loss: 0.6873 - AUC: 0.5459 - val_loss: 0.6916 - val_AUC: 0.5394 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "427/427 [==============================] - 63s 148ms/step - loss: 0.6870 - AUC: 0.5465 - val_loss: 0.6920 - val_AUC: 0.5390 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "427/427 [==============================] - 62s 145ms/step - loss: 0.6869 - AUC: 0.5470 - val_loss: 0.6917 - val_AUC: 0.5391 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "427/427 [==============================] - ETA: 0s - loss: 0.6868 - AUC: 0.5472\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "427/427 [==============================] - 62s 145ms/step - loss: 0.6868 - AUC: 0.5472 - val_loss: 0.6918 - val_AUC: 0.5386 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "427/427 [==============================] - 62s 145ms/step - loss: 0.6867 - AUC: 0.5479 - val_loss: 0.6918 - val_AUC: 0.5390 - lr: 1.0000e-05\n",
            "Epoch 19/100\n",
            "427/427 [==============================] - 64s 149ms/step - loss: 0.6867 - AUC: 0.5476 - val_loss: 0.6919 - val_AUC: 0.5390 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "427/427 [==============================] - ETA: 0s - loss: 0.6867 - AUC: 0.5475\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "427/427 [==============================] - 63s 148ms/step - loss: 0.6867 - AUC: 0.5475 - val_loss: 0.6919 - val_AUC: 0.5391 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "427/427 [==============================] - 63s 148ms/step - loss: 0.6867 - AUC: 0.5474 - val_loss: 0.6918 - val_AUC: 0.5392 - lr: 1.0000e-06\n",
            "[0:22:18] ROC AUC:\t 0.5393780469894409\n"
          ]
        }
      ],
      "source": [
        "start_time_fold = time()\n",
        "\n",
        "ckp_path = 'JSTransformer.hdf5'\n",
        "with strategy.scope():\n",
        "    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n",
        "rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = verbose, \n",
        "                        min_delta = 1e-4, mode = 'max')\n",
        "ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n",
        "                      save_best_only = True, save_weights_only = True, mode = 'max')\n",
        "es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n",
        "                   baseline = None, restore_best_weights = True, verbose = 0)\n",
        "history = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), batch_size = batch_size,\n",
        "                    epochs = 100, callbacks = [rlr, ckp, es], verbose = verbose)\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] ROC AUC:\\t', hist['val_AUC'].max())\n",
        "\n",
        "K.clear_session()\n",
        "del model, X_tr, y_tr\n",
        "rubbish = gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emb1-cSrUKtM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzdDUcuEUKtM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPnMjpISUKtM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Transformer_stock_market_stock_market.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}